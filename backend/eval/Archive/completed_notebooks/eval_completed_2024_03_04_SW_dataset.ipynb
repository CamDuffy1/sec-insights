{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if document exists. If not, get it and save it.\n",
    "import os\n",
    "import requests\n",
    "from app import schema\n",
    "from app.chat.engine import fetch_and_read_document\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "DOCUMENT_PERSIST_PATH = '/workspaces/sec-insights/backend/eval/document_storage/META_SEC_10-K_2022.json'\n",
    "if not os.path.exists(DOCUMENT_PERSIST_PATH):\n",
    "\n",
    "    # Get document from secinsights.ai production API\n",
    "    base_url = \"https://llama-app-backend.onrender.com\"     # Production backend base URL\n",
    "    prod_document_id = \"922f4a9f-7bc9-4fb1-b9b8-fa5a84c1cbcc\"    # META 10-K 2022\n",
    "\n",
    "    response = requests.get(f\"{base_url}/api/document/{prod_document_id}\")\n",
    "    data = response.json()\n",
    "    document = schema.Document(**data)\n",
    "    document = fetch_and_read_document(document)     # merge document pages into a single document\n",
    "\n",
    "    document_docstore = SimpleDocumentStore()\n",
    "    document_docstore.add_documents(document)\n",
    "    document_docstore.persist(DOCUMENT_PERSIST_PATH)\n",
    "    print(f\"Saved document at: {DOCUMENT_PERSIST_PATH}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Document already exists at: {DOCUMENT_PERSIST_PATH}\")\n",
    "    document_docstore = SimpleDocumentStore().from_persist_path(DOCUMENT_PERSIST_PATH)\n",
    "\n",
    "# Get ID of document in docstore\n",
    "docs = document_docstore.docs\n",
    "keys_list = list(docs.keys())\n",
    "doc_id = list(docs.keys())[0]       # Meta document is only document in docstore -- so, first key must be it's ID\n",
    "\n",
    "meta_doc = document_docstore.get_document(doc_id=doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anyio\n",
    "from app.chat.messaging import ChatCallbackHandler\n",
    "\n",
    "send_chan, recv_chan = anyio.create_memory_object_stream(100)\n",
    "callback_handler = ChatCallbackHandler(send_chan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get original node parsing\n",
    "import os\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from app.chat.engine import get_tool_service_context\n",
    "\n",
    "ORIGINAL_NODES_PERSIST_PATH = '/workspaces/sec-insights/backend/eval/node_storage/original_nodes.json'\n",
    "\n",
    "original_service_context = get_tool_service_context(callback_handlers=[callback_handler], node_parser_type=\"original\")\n",
    "original_node_parser = original_service_context.node_parser\n",
    "\n",
    "if not os.path.exists(ORIGINAL_NODES_PERSIST_PATH):\n",
    "    print(f\"Creating original node parsing and saving at: {ORIGINAL_NODES_PERSIST_PATH}\")\n",
    "    original_nodes = original_node_parser.get_nodes_from_documents([meta_doc])\n",
    "    original_docstore = SimpleDocumentStore()\n",
    "    original_docstore.add_documents(original_nodes)\n",
    "    original_docstore.persist(ORIGINAL_NODES_PERSIST_PATH)\n",
    "else:\n",
    "    print(f\"Original node parsing exists - loading it from: {ORIGINAL_NODES_PERSIST_PATH}\")\n",
    "    original_docstore = SimpleDocumentStore().from_persist_path(ORIGINAL_NODES_PERSIST_PATH)\n",
    "    original_document = original_docstore.docs\n",
    "    original_nodes = list(original_document.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total nodes: {len(original_nodes)}\")\n",
    "\n",
    "from eval import format_pdf_text\n",
    "print(f\"\\n{'#'*50} ORIGINAL NODE {'#'*50}\\n{format_pdf_text(original_nodes[5].text)}\")\n",
    "\n",
    "## Additional Output\n",
    "# print(f\"\\ntype(original_nodes): {type(original_nodes)}\")\n",
    "# print(f\"type(original_nodes[0]): {type(original_nodes[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentence-window node parsing\n",
    "\n",
    "SENTENCE_WINDOW_NODES_PERSIST_PATH = '/workspaces/sec-insights/backend/eval/node_storage/sentence_window_nodes.json'\n",
    "\n",
    "sentence_window_service_context = get_tool_service_context(callback_handlers=[callback_handler], node_parser_type=\"sentence-window\")\n",
    "sentence_window_node_parser = sentence_window_service_context.node_parser\n",
    "\n",
    "if not os.path.exists(SENTENCE_WINDOW_NODES_PERSIST_PATH):\n",
    "    print(f\"Creating sentence window node parsing and saving at: {SENTENCE_WINDOW_NODES_PERSIST_PATH}\")\n",
    "    sentence_window_nodes = sentence_window_node_parser.get_nodes_from_documents([meta_doc])\n",
    "    sentence_window_docstore = SimpleDocumentStore()\n",
    "    sentence_window_docstore.add_documents(sentence_window_nodes)\n",
    "    sentence_window_docstore.persist(SENTENCE_WINDOW_NODES_PERSIST_PATH)\n",
    "\n",
    "else:\n",
    "    print(f\"Sentence window node parsing exists - loading it from: {SENTENCE_WINDOW_NODES_PERSIST_PATH}\")\n",
    "    sentence_window_docstore = SimpleDocumentStore().from_persist_path(SENTENCE_WINDOW_NODES_PERSIST_PATH)\n",
    "    sentence_window_document = sentence_window_docstore.docs\n",
    "    sentence_window_nodes = list(sentence_window_document.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total sentence-window nodes: {len(sentence_window_nodes)}\")\n",
    "print(f\"Window_size = {sentence_window_node_parser.window_size}\")\n",
    "\n",
    "sentence = format_pdf_text(sentence_window_nodes[5].metadata.get(\"original_text\"))\n",
    "window = format_pdf_text(sentence_window_nodes[5].metadata.get(\"window\"))\n",
    "\n",
    "print(f\"\\nSentence-Window node:\")\n",
    "print(f\"\\n{'#'*50} SENTENCE {'#'*50}\\n{sentence}\")\n",
    "print(f\"\\n{'#'*50} WINDOW {'#'*50}\\n{window}\")\n",
    "\n",
    "# ## Additional Output\n",
    "# print(f\"\\ntype(sentence_window_nodes): {type(sentence_window_nodes)}\")\n",
    "# print(f\"type(sentence_window_nodes[0]): {type(sentence_window_nodes[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hierarchical node parsing\n",
    "HIERARCHICAL_NODES_PERSIST_PATH = '/workspaces/sec-insights/backend/eval/node_storage/hierarchical_nodes.json'\n",
    "\n",
    "auto_merging_service_context = get_tool_service_context(callback_handlers=[callback_handler], node_parser_type=\"hierarchical\")\n",
    "hierarchical_node_parser = auto_merging_service_context.node_parser\n",
    "\n",
    "if not os.path.exists(HIERARCHICAL_NODES_PERSIST_PATH):\n",
    "    print(f\"Creating hierarchical node parsing and saving at: {HIERARCHICAL_NODES_PERSIST_PATH}\")\n",
    "    hierarchical_nodes = hierarchical_node_parser.get_nodes_from_documents([meta_doc])\n",
    "    hierarchical_docstore = SimpleDocumentStore()\n",
    "    hierarchical_docstore.add_documents(hierarchical_nodes)\n",
    "    hierarchical_docstore.persist(HIERARCHICAL_NODES_PERSIST_PATH)\n",
    "\n",
    "else:\n",
    "    print(f\"Hierarchical node parsing exists - loading it from: {HIERARCHICAL_NODES_PERSIST_PATH}\")\n",
    "    hierarchical_docstore = SimpleDocumentStore().from_persist_path(HIERARCHICAL_NODES_PERSIST_PATH)\n",
    "    hierarchical_document = hierarchical_docstore.docs\n",
    "    hierarchical_nodes = list(hierarchical_document.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import get_leaf_nodes\n",
    "leaf_nodes = get_leaf_nodes(hierarchical_nodes)\n",
    "\n",
    "print(f\"Node hierarchy (chunk sizes): {hierarchical_node_parser.chunk_sizes}\")\n",
    "print(f\"Total hierarchical nodes: {len(hierarchical_nodes)}\")\n",
    "print(f\"Total leaf nodes: {len(leaf_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get parent of a hierarchical node\n",
    "get_parent_node = lambda node, all_nodes: next(i for i in all_nodes if i.id_ == node.parent_node.node_id)\n",
    "\n",
    "# get intermediate & root nodes\n",
    "leaf_node = leaf_nodes[0]\n",
    "intermediate_node = get_parent_node(leaf_node, hierarchical_nodes)\n",
    "root_node = get_parent_node(intermediate_node, hierarchical_nodes)\n",
    "\n",
    "print(f\"Notice how each node is a subset of its parent:\")\n",
    "print(f\"\\n{'#'*50} LEAF NODE {'#'*50}\\n{format_pdf_text(leaf_node.text)}\")\n",
    "print(f\"\\n{'#'*50} INTERMEDIATE NODE {'#'*50}\\n{format_pdf_text(intermediate_node.text)}\")\n",
    "# print(f\"\\n{'#'*50} ROOT NODE {'#'*50}\\n{format_pdf_text(root_node.text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "ORIGINAL_PERSIST_DIR = '/workspaces/sec-insights/backend/eval/index_storage/original'   # local dir to persist storage of index\n",
    "if not os.path.exists(ORIGINAL_PERSIST_DIR):                                            # check if storage already exists\n",
    "    print(f\"Creating Original index and saving it at: {ORIGINAL_PERSIST_DIR}\")\n",
    "    original_index = VectorStoreIndex(original_nodes)                                   # create the index\n",
    "    original_index.storage_context.persist(persist_dir=ORIGINAL_PERSIST_DIR)            # store it for later\n",
    "else:\n",
    "    print(f\"Original index exists - loading it from: {ORIGINAL_PERSIST_DIR}\")\n",
    "    original_storage_context = StorageContext.from_defaults(persist_dir=ORIGINAL_PERSIST_DIR)   # load the existing index\n",
    "    original_index = load_index_from_storage(original_storage_context)\n",
    "\n",
    "SENTENCE_WINDOW_PERSIST_DIR = '/workspaces/sec-insights/backend/eval/index_storage/setence_window'\n",
    "if not os.path.exists(SENTENCE_WINDOW_PERSIST_DIR):\n",
    "    print(f\"Creating Sentence-Window index and saving it at: {SENTENCE_WINDOW_PERSIST_DIR}\")\n",
    "    sentence_window_index = VectorStoreIndex.from_documents(\n",
    "        [meta_doc],\n",
    "        service_context=sentence_window_service_context,\n",
    "    )\n",
    "    sentence_window_index.storage_context.persist(persist_dir=SENTENCE_WINDOW_PERSIST_DIR)\n",
    "else:\n",
    "    print(f\"Sentence-Window index exists - loading it from: {SENTENCE_WINDOW_PERSIST_DIR}\")\n",
    "    setence_window_storage_context = StorageContext.from_defaults(persist_dir=SENTENCE_WINDOW_PERSIST_DIR)\n",
    "    sentence_window_index = load_index_from_storage(\n",
    "        storage_context=setence_window_storage_context,\n",
    "        service_context=sentence_window_service_context,\n",
    "    )\n",
    "\n",
    "AUTO_MERGING_INDEX_PERSIST_DIR = '/workspaces/sec-insights/backend/eval/index_storage/auto_merging_2'\n",
    "if not os.path.exists(AUTO_MERGING_INDEX_PERSIST_DIR):\n",
    "    print(f\"Creating Auto-Merging storage context and saving it at: {AUTO_MERGING_INDEX_PERSIST_DIR}\")\n",
    "    auto_merging_storage_context = StorageContext.from_defaults(\n",
    "        docstore=hierarchical_docstore,\n",
    "    )\n",
    "    auto_merging_index = VectorStoreIndex(\n",
    "        leaf_nodes,\n",
    "        storage_context=auto_merging_storage_context,\n",
    "        service_context=auto_merging_service_context\n",
    "    )\n",
    "    auto_merging_index.storage_context.persist(AUTO_MERGING_INDEX_PERSIST_DIR)\n",
    "else:\n",
    "    print(f\"Auto-Merging index exists - loading it from: {AUTO_MERGING_INDEX_PERSIST_DIR}\")\n",
    "    auto_merging_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=AUTO_MERGING_INDEX_PERSIST_DIR),\n",
    "        service_context=auto_merging_service_context\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Query Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build original query engine\n",
    "original_query_engine = original_index.as_query_engine(\n",
    "    similarity_top_k=3                                      # same as original source code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sentence-window query engine\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor, LLMRerank\n",
    "postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "sentence_window_rerank = LLMRerank(\n",
    "    top_n=2,\n",
    "    service_context=sentence_window_service_context,\n",
    ")\n",
    "sentence_window_query_engine = sentence_window_index.as_query_engine(\n",
    "    similarity_top_k=4, node_postprocessors=[postproc, sentence_window_rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build auto-merging query engine\n",
    "from llama_index.retrievers import AutoMergingRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "base_retriever = auto_merging_index.as_retriever(\n",
    "    similarity_top_k=12,\n",
    ")\n",
    "auto_merging_retriever = AutoMergingRetriever(\n",
    "    base_retriever, StorageContext.from_defaults(persist_dir=AUTO_MERGING_INDEX_PERSIST_DIR),   # base retriever and auto-merging storage context\n",
    ")\n",
    "auto_merging_rerank = LLMRerank(\n",
    "    top_n=4,\n",
    "    service_context=auto_merging_service_context,\n",
    ")\n",
    "auto_merging_query_engine = RetrieverQueryEngine.from_args(\n",
    "    auto_merging_retriever, node_postprocessors=[auto_merging_rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare The Responses of each Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is Meta's mission?\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Excerpt from META 2022 10-K Document: Our mission is to give people the power to build community and bring the world closer together.\")\n",
    "\n",
    "original_response = original_query_engine.query(prompt)\n",
    "sentence_window_response = sentence_window_query_engine.query(prompt)\n",
    "auto_merging_response = auto_merging_query_engine.query(prompt)\n",
    "\n",
    "print(f\"\\nORIGINAL QUERY ENGINE RESPONSE:\\n{str(original_response)}\")\n",
    "print(f\"\\nSENTENCE-WINDOW QUERY ENGINE RESPONSE:\\n{str(sentence_window_response)}\")\n",
    "print(f\"\\nAUTO-MERGING QUERY ENGINE RESPONSE:\\n{str(auto_merging_response)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()    # allow asynchonous code to run within Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import generate_dataset\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index.evaluation import DatasetGenerator, QueryResponseDataset\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "import random\n",
    "\n",
    "file_path=\"/workspaces/sec-insights/backend/eval/eval_dataset.json\"     # path to save evaluation dataset\n",
    "if not os.path.exists(file_path):\n",
    "    text_splitter = SentenceSplitter()\n",
    "    base_nodes = text_splitter.get_nodes_from_documents(document)\n",
    "\n",
    "    # Use the middle 80% of document context to generate questions in evaluation dataset\n",
    "    start_index = int(len(base_nodes) * 0.1)\n",
    "    end_index = int(len(base_nodes) * 0.9)\n",
    "\n",
    "    num_nodes_eval = 30    #  The number of nodes (randomly sampled from total nodes) to use for generating evaluation questions.\n",
    "    sample_eval_nodes = random.sample(base_nodes[start_index:end_index], num_nodes_eval)\n",
    "\n",
    "    dataset_generator_llm = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "    dataset_generator_service_context = ServiceContext.from_defaults(llm=dataset_generator_llm)\n",
    "    \n",
    "    dataset_generator = DatasetGenerator(\n",
    "        nodes=sample_eval_nodes,\n",
    "        # service_context=original_service_context,\n",
    "        service_context=sentence_window_service_context,\n",
    "        # service_context=dataset_generator_service_context,        # rate limiting issues using gpt-4\n",
    "        num_questions_per_chunk=2,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    eval_dataset = await dataset_generator.agenerate_dataset_from_nodes()\n",
    "    eval_dataset.save_json(file_path)\n",
    "    print(f\"Saved evaluation dataset at: {file_path}\")\n",
    "\n",
    "else: \n",
    "    print(f\"Evaluation dataset already exists at: {file_path}\")\n",
    "    eval_dataset = QueryResponseDataset.from_json(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Query Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_='''\n",
    "Note - The following code snippet had to be added to the CorrectnessEvaluator class within the llama-index v\"0.9.7\" package at\n",
    "llama_index/evaluation/correctness.py before line: score_str, reasoning_str = eval_response.split(\"\\n\", 1)\n",
    "This resolves an issue where eval_resonse is created beginning with a newline character, resulting in an error trying to convert\n",
    "an empty str to a float on line: score = float(score_str).\n",
    "\n",
    "Code snippet:\n",
    "if eval_response[0] == '\\n':\n",
    "    eval_response = eval_response[1:]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"/workspaces/sec-insights/backend/eval/eval_dataset_SW_service_context.json\"\n",
    "file_path=\"/workspaces/sec-insights/backend/eval/eval_dataset_SW_service_context.json\"\n",
    "eval_dataset = QueryResponseDataset.from_json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluate responses based on the following metrics:\n",
    "    Correctness:            The correctness of a response - A score between 1 (worst) and 5 (best).\n",
    "    Semantic Similarity:    The similarity between embeddings of the generated answer and reference answer.\n",
    "    Relevance:              The relevance of retrieved context and response to the query. Considers the query string, retrieved context, and response string.\n",
    "    Faithfulness:           How well the response is supported by the retrieved context (i.e., Is there hallucination?)\n",
    "'''\n",
    "from llama_index.evaluation import CorrectnessEvaluator, SemanticSimilarityEvaluator, RelevancyEvaluator, FaithfulnessEvaluator, BatchEvalRunner\n",
    "from llama_index.evaluation.eval_utils import get_responses\n",
    "\n",
    "evaluator_c = CorrectnessEvaluator()\n",
    "evaluator_s = SemanticSimilarityEvaluator()\n",
    "evaluator_r = RelevancyEvaluator()\n",
    "evaluator_f = FaithfulnessEvaluator()\n",
    "\n",
    "evaluator_dict = {\n",
    "    \"correctness\": evaluator_c,\n",
    "    \"faithfulness\": evaluator_f,\n",
    "    \"relevancy\": evaluator_r,\n",
    "    \"semantic_similarity\": evaluator_s,\n",
    "}\n",
    "batch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)\n",
    "\n",
    "eval_qs = eval_dataset.questions\n",
    "ref_response_strs = [r for (_, r) in eval_dataset.qr_pairs]\n",
    "print(f\"{len(eval_qs)} questions in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in eval_qs[:3]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Responses from Each Query Engine for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 20\n",
    "queries = list(eval_dataset.queries.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Query Engine\")\n",
    "i = 0\n",
    "original_pred_responses = []\n",
    "while i < max_samples:\n",
    "    try:\n",
    "        print(f\"Making prediction {i + 1}/{max_samples}\", end='\\r', flush=True)\n",
    "        prompt = queries[i]\n",
    "        response = original_query_engine.query(prompt)\n",
    "        original_pred_responses.append(response)\n",
    "        i +=1\n",
    "    except Exception as e:\n",
    "        # Catch-all to handle exceptions\n",
    "        print(f\"Exception occured: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_pred_responses = get_responses(\n",
    "#     eval_qs[:max_samples], original_query_engine, show_progress=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import RateLimitError\n",
    "print(f\"Sentence-Window Query Engine\")\n",
    "i = 0\n",
    "sentence_window_pred_responses = []\n",
    "while i < max_samples:\n",
    "    try:\n",
    "        print(f\"Making prediction {i + 1}/{max_samples}\", end='\\r', flush=True)\n",
    "        prompt = queries[i]\n",
    "        response = sentence_window_query_engine.query(prompt)\n",
    "        sentence_window_pred_responses.append(response)\n",
    "        i +=1\n",
    "        \n",
    "    except RateLimitError as rate_limit_err:\n",
    "        print(rate_limit_err)\n",
    "        sleep = 12\n",
    "        for s in range(sleep):\n",
    "            print(f\"waiting {s}/{sleep}s\", end='\\r', flush=True)\n",
    "            time.sleep(sleep)\n",
    "    except Exception as e:  # Catch-all to handle exceptions\n",
    "        print(f\"Exception occured: {e}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Sentence-Window Query Engine\")\n",
    "# sentence_window_pred_responses = get_responses(\n",
    "#     eval_qs[:max_samples], sentence_window_query_engine, show_progress=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "auto_merging_pred_responses = []\n",
    "while i <max_samples:\n",
    "    try:\n",
    "        print(f\"Making prediction {i + 1}/{max_samples}\", end='\\r', flush=True)\n",
    "        prompt = queries[i]\n",
    "        response = auto_merging_query_engine.query(prompt)\n",
    "        auto_merging_pred_responses.append(response)\n",
    "        i +=1\n",
    "\n",
    "    except RateLimitError as rate_limit_err:\n",
    "        print(rate_limit_err)\n",
    "        sleep = 12\n",
    "        for s in range(sleep):\n",
    "            print(f\"waiting {s}/{sleep}s\", end='\\r', flush=True)\n",
    "            time.sleep(1)\n",
    "    except Exception as e:  # Catch-all to handle exceptions\n",
    "        print(f\"Exception occured: {e}\")\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Auto-Merging Query Engine\")\n",
    "# auto_merging_pred_responses = get_responses(\n",
    "#     eval_qs[:max_samples], auto_merging_query_engine, show_progress=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Responses from Each Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(12)\n",
    "print(f\"Original Query Engine\")\n",
    "original_eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs[:max_samples],\n",
    "    responses=original_pred_responses[:max_samples],\n",
    "    reference=ref_response_strs[:max_samples],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)\n",
    "print(f\"Sentence-Window Query Engine\")\n",
    "sentence_window_eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs[:max_samples],\n",
    "    responses=sentence_window_pred_responses[:max_samples],\n",
    "    reference=ref_response_strs[:max_samples],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)\n",
    "print(f\"Auto-Merging Query Engine\")\n",
    "auto_merging_eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs[:max_samples],\n",
    "    responses=auto_merging_pred_responses[:max_samples],\n",
    "    reference=ref_response_strs[:max_samples],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation.eval_utils import get_results_df\n",
    "results_df = get_results_df(\n",
    "    [original_eval_results, sentence_window_eval_results, auto_merging_eval_results],\n",
    "    [\"Base Retriever\", \"Sentence-Window Retriever\", \"Auto-Merging Retriever\"],\n",
    "    [\"correctness\", \"relevancy\", \"faithfulness\", \"semantic_similarity\"],\n",
    ")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to CSV file\n",
    "OUTPUT_PATH = '/workspaces/sec-insights/backend/eval/results/results_2024_03_04_samples-20_SW_dataset.csv'\n",
    "results_df.to_csv(OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
