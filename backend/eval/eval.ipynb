{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '922f4a9f-7bc9-4fb1-b9b8-fa5a84c1cbcc',\n",
       " 'created_at': '2023-11-03T22:42:50.306199',\n",
       " 'updated_at': '2023-11-03T22:42:50.306199',\n",
       " 'url': 'https://d687lz8k56fia.cloudfront.net/sec-edgar-filings/0001326801/10-K/0001326801-23-000013/primary-document.pdf',\n",
       " 'metadata_map': {'sec_document': {'cik': '0001326801',\n",
       "   'year': 2022,\n",
       "   'doc_type': '10-K',\n",
       "   'company_name': 'Meta Platforms, Inc.',\n",
       "   'company_ticker': 'META',\n",
       "   'accession_number': '0001326801-23-000013',\n",
       "   'filed_as_of_date': '2023-02-02T00:00:00',\n",
       "   'date_as_of_change': '2023-02-01T00:00:00',\n",
       "   'period_of_report_date': '2022-12-31T00:00:00'}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get document from secinsights.ai production API (avoids having to seed db each time running this notebook)\n",
    "import requests\n",
    "base_url = \"https://llama-app-backend.onrender.com\"     # Production backend base URL\n",
    "document_id = \"922f4a9f-7bc9-4fb1-b9b8-fa5a84c1cbcc\"    # META 10-K 2022\n",
    "\n",
    "response = requests.get(f\"{base_url}/api/document/{document_id}\")\n",
    "data = response.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id=UUID('922f4a9f-7bc9-4fb1-b9b8-fa5a84c1cbcc'), created_at=datetime.datetime(2023, 11, 3, 22, 42, 50, 306199), updated_at=datetime.datetime(2023, 11, 3, 22, 42, 50, 306199), url='https://d687lz8k56fia.cloudfront.net/sec-edgar-filings/0001326801/10-K/0001326801-23-000013/primary-document.pdf', metadata_map={<DocumentMetadataKeysEnum.SEC_DOCUMENT: 'sec_document'>: {'cik': '0001326801', 'year': 2022, 'doc_type': '10-K', 'company_name': 'Meta Platforms, Inc.', 'company_ticker': 'META', 'accession_number': '0001326801-23-000013', 'filed_as_of_date': '2023-02-02T00:00:00', 'date_as_of_change': '2023-02-01T00:00:00', 'period_of_report_date': '2022-12-31T00:00:00'}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app import schema\n",
    "document = schema.Document(**data)\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eval import get_dummy_doc\n",
    "# document = get_dummy_doc()\n",
    "# document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DOCUMENT WITH SimpleDirectoryReader: META 10-K 2022\n",
      "MERGING DOCUMENT: META 10-K 2022\n"
     ]
    }
   ],
   "source": [
    "from app.chat.engine import fetch_and_read_document\n",
    "document = fetch_and_read_document(document)     # merge document pages into a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(document): 1\n",
      "type(document): <class 'list'>\n",
      "type(document[0]): <class 'llama_index.schema.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(document): {len(document)}\")\n",
    "print(f\"type(document): {type(document)}\")\n",
    "print(f\"type(document[0]): {type(document[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anyio\n",
    "from app.chat.messaging import ChatCallbackHandler\n",
    "\n",
    "send_chan, recv_chan = anyio.create_memory_object_stream(100)\n",
    "callback_handler = ChatCallbackHandler(send_chan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### CREATING SERVICE CONTEXT USING original NODE PARSER #########################\n"
     ]
    }
   ],
   "source": [
    "from app.chat.engine import get_tool_service_context\n",
    "original_service_context = get_tool_service_context(callback_handlers=[callback_handler], node_parser_type=\"original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 327\n",
      "\n",
      "################################################## ORIGINAL NODE ##################################################\n",
      "We have based these forward-looking statements largely on our current expectations and projections about future events and trends that we believe may affect our financial condition, results of operations, business strategy, short-term and long-term business operations and objectives, and financial needs. These forward-looking statements are subject to a number of risks, uncertainties and assumptions, including those described in Part I, Item 1A, \"Risk Factors\" in this Annual Report on Form 10-K. Moreover, we operate in a very competitive and rapidly changing environment. New risks emerge from time to time. It is not possible for our management to predict all risks, nor can we assess the impact of all factors on our business or the extent to which any factor, or combination of factors, may cause actual results to differ materially from those contained in any forward-looking statements we may make. In light of these risks, uncertainties and assumptions, the future events and trends discussed in this Annual Report on Form 10-K may not occur and actual results could differ materially and adversely from those anticipated or implied in the forward-looking statements. We undertake no obligation to revise or publicly release the results of any revision to these forward-looking statements, except as required by law. Given these risks and uncertainties, readers are cautioned not to place undue reliance on such forward-looking statements.\n"
     ]
    }
   ],
   "source": [
    "# display original node parsing\n",
    "original_node_parser = original_service_context.node_parser\n",
    "original_nodes = original_node_parser.get_nodes_from_documents(document)\n",
    "print(f\"Total nodes: {len(original_nodes)}\")\n",
    "\n",
    "from eval import format_pdf_text\n",
    "print(f\"\\n{'#'*50} ORIGINAL NODE {'#'*50}\\n{format_pdf_text(original_nodes[5].text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### CREATING SERVICE CONTEXT USING sentence-window NODE PARSER #########################\n",
      "Total sentence-window nodes: 2184\n",
      "window_size = 2\n",
      "\n",
      "Sentence-Window node:\n",
      "\n",
      "################################################## SENTENCE ##################################################\n",
      "Yes ☐ No ☒ Indicate by check mark whether the registrant (1) has filed all reports required to be filed by Section 13 or 15(d) of the Securities Exchange Act of 1934 (Exchange Act) during the preceding 12 months (or for such shorter period that the registrant was required to file such reports), and (2) has been subject to such filing requirements for the past 90 days.\n",
      "\n",
      "################################################## WINDOW ##################################################\n",
      "Employer Identification Number) 1601 Willow Road , Menlo Park , California 94025 (Address of principal executive offices and Zip Code) ( 650 ) 543-4800 (Registrant's telephone number, including area code) __________________________ Securities registered pursuant to Section 12(b) of the Act: Title of each class Trading symbol(s) Name of each exchange on which registered Class A Common Stock, $0.000006 par value META The Nasdaq Stock Market LLC Securities registered pursuant to Section 12(g) of the Act: None Indicate by check mark if the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act. Yes ☒ No ☐ Indicate by check mark if the registrant is not required to file reports pursuant to Section 13 or Section 15(d) of the Act. Yes ☐ No ☒ Indicate by check mark whether the registrant (1) has filed all reports required to be filed by Section 13 or 15(d) of the Securities Exchange Act of 1934 (Exchange Act) during the preceding 12 months (or for such shorter period that the registrant was required to file such reports), and (2) has been subject to such filing requirements for the past 90 days. Yes ☒ No ☐ Indicate by check mark whether the registrant has submitted electronically every Interactive Data File required to be submitted pursuant to Rule 405 of Regulation S-T (§ 232.405 of this chapter) during the preceding 12 months (or for such shorter period that the registrant was required to submit such files).\n"
     ]
    }
   ],
   "source": [
    "sentence_window_service_context = get_tool_service_context(callback_handlers=[callback_handler], node_parser_type=\"sentence-window\")\n",
    "sentence_window_node_parser = sentence_window_service_context.node_parser\n",
    "sentence_window_nodes = sentence_window_node_parser.get_nodes_from_documents(document)\n",
    "\n",
    "print(f\"Total sentence-window nodes: {len(sentence_window_nodes)}\")\n",
    "sentence = format_pdf_text(sentence_window_nodes[5].metadata.get(\"original_text\"))\n",
    "window = format_pdf_text(sentence_window_nodes[5].metadata.get(\"window\"))\n",
    "\n",
    "print(f\"window_size = {sentence_window_node_parser.window_size}\")\n",
    "\n",
    "print(f\"\\nSentence-Window node:\")\n",
    "print(f\"\\n{'#'*50} SENTENCE {'#'*50}\\n{sentence}\")\n",
    "print(f\"\\n{'#'*50} WINDOW {'#'*50}\\n{window}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### CREATING SERVICE CONTEXT USING hierarchical NODE PARSER #########################\n"
     ]
    }
   ],
   "source": [
    "# parse nodes hierarchically\n",
    "from llama_index.node_parser import get_leaf_nodes, get_root_nodes\n",
    "auto_merging_service_context = get_tool_service_context(callback_handlers=[callback_handler], node_parser_type=\"hierarchical\")\n",
    "\n",
    "hierarchical_node_parser = auto_merging_service_context.node_parser\n",
    "hierarchical_nodes = hierarchical_node_parser.get_nodes_from_documents(document)\n",
    "leaf_nodes = get_leaf_nodes(hierarchical_nodes)\n",
    "root_nodes = get_root_nodes(hierarchical_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node hierarchy (chunk sizes): [2048, 512, 128]\n",
      "Total hierarchical nodes: 2556\n",
      "Total leaf nodes: 2119\n"
     ]
    }
   ],
   "source": [
    "print(f\"Node hierarchy (chunk sizes): {hierarchical_node_parser.chunk_sizes}\")\n",
    "print(f\"Total hierarchical nodes: {len(hierarchical_nodes)}\")\n",
    "print(f\"Total leaf nodes: {len(leaf_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice how each node is a subset of its parent:\n",
      "\n",
      "################################################## LEAF NODE ##################################################\n",
      "UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington, D.C.\n",
      "\n",
      "################################################## INTERMEDIATE NODE ##################################################\n",
      "UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington, D.C. 20549 __________________________ FORM 10-K __________________________ (Mark One) ☒ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934 For the fiscal year ended December 31 , 2022 or ☐ TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934 For the transition period from to Commission File Number: 001-35551 __________________________ Meta Platforms, Inc. (Exact name of registrant as specified in its charter) __________________________ Delaware 20-1665019 (State or other jurisdiction of incorporation or organization) (I.R.S. Employer Identification Number) 1601 Willow Road , Menlo Park , California 94025 (Address of principal executive offices and Zip Code) ( 650 ) 543-4800 (Registrant's telephone number, including area code) __________________________ Securities registered pursuant to Section 12(b) of the Act: Title of each class Trading symbol(s) Name of each exchange on which registered Class A Common Stock, $0.000006 par value META The Nasdaq Stock Market LLC Securities registered pursuant to Section 12(g) of the Act: None Indicate by check mark if the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act.\n",
      "\n",
      "################################################## ROOT NODE ##################################################\n",
      "UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington, D.C. 20549 __________________________ FORM 10-K __________________________ (Mark One) ☒ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934 For the fiscal year ended December 31 , 2022 or ☐ TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934 For the transition period from to Commission File Number: 001-35551 __________________________ Meta Platforms, Inc. (Exact name of registrant as specified in its charter) __________________________ Delaware 20-1665019 (State or other jurisdiction of incorporation or organization) (I.R.S. Employer Identification Number) 1601 Willow Road , Menlo Park , California 94025 (Address of principal executive offices and Zip Code) ( 650 ) 543-4800 (Registrant's telephone number, including area code) __________________________ Securities registered pursuant to Section 12(b) of the Act: Title of each class Trading symbol(s) Name of each exchange on which registered Class A Common Stock, $0.000006 par value META The Nasdaq Stock Market LLC Securities registered pursuant to Section 12(g) of the Act: None Indicate by check mark if the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act. Yes ☒ No ☐ Indicate by check mark if the registrant is not required to file reports pursuant to Section 13 or Section 15(d) of the Act. Yes ☐ No ☒ Indicate by check mark whether the registrant (1) has filed all reports required to be filed by Section 13 or 15(d) of the Securities Exchange Act of 1934 (Exchange Act) during the preceding 12 months (or for such shorter period that the registrant was required to file such reports), and (2) has been subject to such filing requirements for the past 90 days. Yes ☒ No ☐ Indicate by check mark whether the registrant has submitted electronically every Interactive Data File required to be submitted pursuant to Rule 405 of Regulation S-T (§ 232.405 of this chapter) during the preceding 12 months (or for such shorter period that the registrant was required to submit such files). Yes ☒ No ☐ Indicate by check mark whether the registrant is a large accelerated filer, an accelerated filer, a non-accelerated filer, a smaller reporting company, or an emerging growth company. See the definitions of \"large accelerated filer,\" \"accelerated filer,\" \"smaller reporting company,\" and \"emerging growth company\" in Rule 12b-2 of the Exchange Act. Large accelerated filer ☒ Accelerated filer ☐ Non-accelerated filer ☐ Smaller reporting company ☐ Emerging growth company ☐ If an emerging growth company, indicate by check mark if the registrant has elected not to use the extended transition period for complying with any new or revised financial accounting standards provided pursuant to Section 13(a) of the Exchange Act. ☐ Indicate by check mark whether the registrant has filed a report on and attestation to its management's assessment of the effectiveness of its internal control over financial reporting under Section 404(b) of the Sarbanes-Oxley Act (15 U.S.C. 7262(b)) by the registered public accounting firm that prepared or issued its audit report. ☒ If securities are registered pursuant to Section 12(b) of the Act, indicate by check mark whether the financial statements of the registrant included in the filing reflect the correction of an error to previously issued financial statements. ☐ Indicate by check mark whether any of those error corrections are restatements that required a recovery analysis of incentive-based compensation received by any of the registrant’s executive officers during the relevant recovery period pursuant to §240.10D-1(b). ☐ Indicate by check mark whether the registrant is a shell company (as defined in Rule 12b-2 of the Exchange Act). Yes ☐ No ☒ The aggregate market value of the voting and non-voting stock held by non-affiliates of the registrant as of June 30, 2022, the last business day of the registrant's most recently completed second fiscal quarter, was $ 378 billion based upon the closing price reported for such date on the Nasdaq Global Select Market. On January 27, 2023, the registrant had 2,225,763,078 shares of Class A common stock and 366,876,470 shares of Class B common stock outstanding. DOCUMENTS INCORPORATED BY REFERENCE Portions of the registrant's Proxy Statement for the 2023 Annual Meeting of Stockholders are incorporated herein by reference in Part III of this Annual Report on Form 10-K to the extent stated herein. Such proxy statement will be filed with the Securities and Exchange Commission within 120 days of the registrant's fiscal year ended December 31, 2022. Meta Platforms, Inc. Form 10-K TABLE OF CONTENTS Page Note About Forward-Looking Statements 3 Limitations of Key Metrics and Other Data 4 PART I Item 1. Business 7 Item 1A. Risk Factors 14 Item 1B. Unresolved Staff Comments 48 Item 2. Properties 48 Item 3. Legal Proceedings 48 Item 4. Mine Safety Disclosures 51 PART II Item 5. Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities 52 Item 6. [Reserved] 53 Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations 54 Item 7A. Quantitative and Qualitative Disclosures About Market Risk 78 Item 8. Financial Statements and Supplementary Data 80 Item 9. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure 120 Item 9A. Controls and Procedures 120 Item 9B. Other Information 120 Item 9C. Disclosure Regarding Foreign Jurisdictions that Prevent Inspections 120 PART III Item 10. Directors, Executive Officers and Corporate Governance 121 Item 11. Executive Compensation 121 Item 12. Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters 121 Item 13. Certain Relationships and Related Transactions, and Director Independence 121 Item 14. Principal Account ant Fees and Services 121 PART IV Item 15. Exhibit and Financial Statement Schedules 122 Item 16.\n"
     ]
    }
   ],
   "source": [
    "# function to get parent of a hierarchical node\n",
    "get_parent_node = lambda node, all_nodes: next(i for i in all_nodes if i.id_ == node.parent_node.node_id)\n",
    "\n",
    "# get intermediate & root nodes\n",
    "leaf_node = leaf_nodes[0]\n",
    "intermediate_node = get_parent_node(leaf_node, hierarchical_nodes)\n",
    "root_node = get_parent_node(intermediate_node, hierarchical_nodes)\n",
    "\n",
    "print(f\"Notice how each node is a subset of its parent:\")\n",
    "print(f\"\\n{'#'*50} LEAF NODE {'#'*50}\\n{format_pdf_text(leaf_node.text)}\")\n",
    "print(f\"\\n{'#'*50} INTERMEDIATE NODE {'#'*50}\\n{format_pdf_text(intermediate_node.text)}\")\n",
    "print(f\"\\n{'#'*50} ROOT NODE {'#'*50}\\n{format_pdf_text(root_node.text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original index exists - loading it from: /workspaces/sec-insights/backend/eval/index_storage/original\n",
      "Sentence-Window index exists - loading it from: /workspaces/sec-insights/backend/eval/index_storage/setence_window\n",
      "Auto-Merging index exists - loading it from: /workspaces/sec-insights/backend/eval/index_storage/auto_merging\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "ORIGINAL_PERSIST_DIR = '/workspaces/sec-insights/backend/eval/index_storage/original'   # local dir to persist storage of index\n",
    "if not os.path.exists(ORIGINAL_PERSIST_DIR):                                            # check if storage already exists\n",
    "    print(f\"Creating Original index and saving it at: {ORIGINAL_PERSIST_DIR}\")\n",
    "    original_index = VectorStoreIndex(original_nodes)                                   # create the index\n",
    "    original_index.storage_context.persist(persist_dir=ORIGINAL_PERSIST_DIR)            # store it for later\n",
    "else:\n",
    "    print(f\"Original index exists - loading it from: {ORIGINAL_PERSIST_DIR}\")\n",
    "    original_storage_context = StorageContext.from_defaults(persist_dir=ORIGINAL_PERSIST_DIR)   # load the existing index\n",
    "    original_index = load_index_from_storage(original_storage_context)\n",
    "\n",
    "SENTENCE_WINDOW_PERSIST_DIR = '/workspaces/sec-insights/backend/eval/index_storage/setence_window'\n",
    "if not os.path.exists(SENTENCE_WINDOW_PERSIST_DIR):\n",
    "    print(f\"Creating Sentence-Window index and saving it at: {SENTENCE_WINDOW_PERSIST_DIR}\")\n",
    "    sentence_window_index = VectorStoreIndex.from_documents(\n",
    "        document,\n",
    "        service_context=sentence_window_service_context,\n",
    "    )\n",
    "    sentence_window_index.storage_context.persist(persist_dir=SENTENCE_WINDOW_PERSIST_DIR)\n",
    "else:\n",
    "    print(f\"Sentence-Window index exists - loading it from: {SENTENCE_WINDOW_PERSIST_DIR}\")\n",
    "    setence_window_storage_context = StorageContext.from_defaults(persist_dir=SENTENCE_WINDOW_PERSIST_DIR)\n",
    "    sentence_window_index = load_index_from_storage(\n",
    "        storage_context=setence_window_storage_context,\n",
    "        service_context=sentence_window_service_context,\n",
    "    )\n",
    "\n",
    "AUTO_MERGING_PERSIST_DIR = '/workspaces/sec-insights/backend/eval/index_storage/auto_merging'\n",
    "auto_merging_storage_context = StorageContext.from_defaults()\n",
    "auto_merging_storage_context.docstore.add_documents(hierarchical_nodes)\n",
    "if not os.path.exists(AUTO_MERGING_PERSIST_DIR):\n",
    "    print(f\"Creating Auto-Merging index and saving it at: {AUTO_MERGING_PERSIST_DIR}\")\n",
    "    auto_merging_index = VectorStoreIndex(\n",
    "        leaf_nodes,\n",
    "        storage_context=auto_merging_storage_context,\n",
    "        service_context=auto_merging_service_context\n",
    "    )\n",
    "    auto_merging_index.storage_context.persist(AUTO_MERGING_PERSIST_DIR)\n",
    "else:\n",
    "    print(f\"Auto-Merging index exists - loading it from: {AUTO_MERGING_PERSIST_DIR}\")\n",
    "    auto_merging_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=AUTO_MERGING_PERSIST_DIR),\n",
    "        service_context=auto_merging_service_context\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Query Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build original query engine\n",
    "original_query_engine = original_index.as_query_engine(\n",
    "    similarity_top_k=3                                      # same as original source code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sentence-window query engine\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor, LLMRerank\n",
    "postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "sentence_window_rerank = LLMRerank(\n",
    "    top_n=2,\n",
    "    service_context=sentence_window_service_context,\n",
    ")\n",
    "sentence_window_query_engine = sentence_window_index.as_query_engine(\n",
    "    similarity_top_k=4, node_postprocessors=[postproc, sentence_window_rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build auto-merging query engine\n",
    "from llama_index.retrievers import AutoMergingRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "base_retriever = auto_merging_index.as_retriever(\n",
    "    similarity_top_k=12,\n",
    ")\n",
    "auto_merging_retriever = AutoMergingRetriever(\n",
    "    base_retriever, auto_merging_storage_context,\n",
    ")\n",
    "auto_merging_rerank = LLMRerank(\n",
    "    top_n=4,\n",
    "    service_context=auto_merging_service_context,\n",
    ")\n",
    "auto_merging_query_engine = RetrieverQueryEngine.from_args(\n",
    "    auto_merging_retriever, node_postprocessors=[auto_merging_rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare The Responses of each Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is Meta's mission?\n",
      "Excerpt from META 2022 10-K Document: Our mission is to give people the power to build community and bring the world closer together.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "doc_id 8c0074b7-64f9-4402-b271-9ffbd54c6a70 not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcerpt from META 2022 10-K Document: Our mission is to give people the power to build community and bring the world closer together.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# original_response = original_query_engine.query(prompt)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# sentence_window_response = sentence_window_query_engine.query(prompt)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m auto_merging_response \u001b[38;5;241m=\u001b[39m \u001b[43mauto_merging_query_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# print(f\"\\nORIGINAL QUERY ENGINE RESPONSE:\\n{str(original_response)}\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(f\"\\nSENTENCE-WINDOW QUERY ENGINE RESPONSE:\\n{str(sentence_window_response)}\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAUTO-MERGING QUERY ENGINE RESPONSE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(auto_merging_response)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llama-app-backend--Qk0ygDj-py3.11/lib/python3.11/site-packages/llama_index/core/base_query_engine.py:30\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     29\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llama-app-backend--Qk0ygDj-py3.11/lib/python3.11/site-packages/llama_index/query_engine/retriever_query_engine.py:170\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    168\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    169\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m--> 170\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[1;32m    172\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[1;32m    173\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    176\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llama-app-backend--Qk0ygDj-py3.11/lib/python3.11/site-packages/llama_index/query_engine/retriever_query_engine.py:126\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m--> 126\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_node_postprocessors(nodes, query_bundle\u001b[38;5;241m=\u001b[39mquery_bundle)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llama-app-backend--Qk0ygDj-py3.11/lib/python3.11/site-packages/llama_index/core/base_retriever.py:54\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m     51\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE,\n\u001b[1;32m     52\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[1;32m     53\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[0;32m---> 54\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m         retrieve_event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m     56\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mNODES: nodes},\n\u001b[1;32m     57\u001b[0m         )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llama-app-backend--Qk0ygDj-py3.11/lib/python3.11/site-packages/llama_index/retrievers/auto_merging_retriever.py:165\u001b[0m, in \u001b[0;36mAutoMergingRetriever._retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve nodes given query.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mImplemented by the user.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m initial_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_retriever\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 165\u001b[0m cur_nodes, is_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_merging\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# cur_nodes, is_changed = self._get_parents_and_merge(initial_nodes)\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m is_changed:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llama-app-backend--Qk0ygDj-py3.11/lib/python3.11/site-packages/llama_index/retrievers/auto_merging_retriever.py:152\u001b[0m, in \u001b[0;36mAutoMergingRetriever._try_merging\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Try different ways to merge nodes.\"\"\"\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# first try filling in nodes\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m nodes, is_changed_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill_in_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# then try merging nodes\u001b[39;00m\n\u001b[1;32m    154\u001b[0m nodes, is_changed_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parents_and_merge(nodes)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llama-app-backend--Qk0ygDj-py3.11/lib/python3.11/site-packages/llama_index/retrievers/auto_merging_retriever.py:128\u001b[0m, in \u001b[0;36mAutoMergingRetriever._fill_in_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    124\u001b[0m     cur_node\u001b[38;5;241m.\u001b[39mnext_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m cur_node\u001b[38;5;241m.\u001b[39mnext_node \u001b[38;5;241m==\u001b[39m nodes[idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mprev_node\n\u001b[1;32m    126\u001b[0m ):\n\u001b[1;32m    127\u001b[0m     is_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     next_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_document\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcur_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_id\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     next_node \u001b[38;5;241m=\u001b[39m cast(BaseNode, next_node)\n\u001b[1;32m    133\u001b[0m     next_node_text \u001b[38;5;241m=\u001b[39m truncate_text(next_node\u001b[38;5;241m.\u001b[39mget_text(), \u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llama-app-backend--Qk0ygDj-py3.11/lib/python3.11/site-packages/llama_index/storage/docstore/keyval_docstore.py:120\u001b[0m, in \u001b[0;36mKVDocumentStore.get_document\u001b[0;34m(self, doc_id, raise_error)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: doc_id 8c0074b7-64f9-4402-b271-9ffbd54c6a70 not found."
     ]
    }
   ],
   "source": [
    "prompt = \"What is Meta's mission?\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Excerpt from META 2022 10-K Document: Our mission is to give people the power to build community and bring the world closer together.\")\n",
    "\n",
    "# original_response = original_query_engine.query(prompt)\n",
    "# sentence_window_response = sentence_window_query_engine.query(prompt)\n",
    "auto_merging_response = auto_merging_query_engine.query(prompt)\n",
    "\n",
    "# print(f\"\\nORIGINAL QUERY ENGINE RESPONSE:\\n{str(original_response)}\")\n",
    "# print(f\"\\nSENTENCE-WINDOW QUERY ENGINE RESPONSE:\\n{str(sentence_window_response)}\")\n",
    "print(f\"\\nAUTO-MERGING QUERY ENGINE RESPONSE:\\n{str(auto_merging_response)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()    # allow asynchonous code to run within Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import generate_dataset\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index.evaluation import DatasetGenerator, QueryResponseDataset\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "import random\n",
    "\n",
    "file_path=\"/workspaces/sec-insights/backend/eval/eval_dataset.json\"     # path to save evaluation dataset\n",
    "if not os.path.exists(file_path):\n",
    "    text_splitter = SentenceSplitter()\n",
    "    base_nodes = text_splitter.get_nodes_from_documents(document)\n",
    "\n",
    "    # Use the middle 80% of document context to generate questions in evaluation dataset\n",
    "    start_index = int(len(base_nodes) * 0.1)\n",
    "    end_index = int(len(base_nodes) * 0.9)\n",
    "\n",
    "    num_nodes_eval = 30    #  The number of nodes (randomly sampled from total nodes) to use for generating evaluation questions.\n",
    "    sample_eval_nodes = random.sample(base_nodes[start_index:end_index], num_nodes_eval)\n",
    "\n",
    "    dataset_generator_llm = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "    dataset_generator_service_context = ServiceContext.from_defaults(llm=dataset_generator_llm)\n",
    "    \n",
    "    dataset_generator = DatasetGenerator(\n",
    "        nodes=sample_eval_nodes,\n",
    "        # service_context=original_service_context,\n",
    "        # service_context=dataset_generator_service_context,        # rate limiting issues using gpt-4\n",
    "        num_questions_per_chunk=2,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    eval_dataset = await dataset_generator.agenerate_dataset_from_nodes()\n",
    "    eval_dataset.save_json(file_path)\n",
    "    print(f\"Saved evaluation dataset at: {file_path}\")\n",
    "\n",
    "    # dataset_generator_SW_service_context = DatasetGenerator(\n",
    "    #     nodes=sample_eval_nodes,\n",
    "    #     service_context=sentence_window_service_context,\n",
    "    #     num_questions_per_chunk=2,\n",
    "    #     show_progress=True,\n",
    "    # )\n",
    "    # eval_dataset_SW_service_context = await dataset_generator_SW_service_context.agenerate_dataset_from_nodes()\n",
    "    # eval_dataset_SW_service_context.save_json(\"/workspaces/sec-insights/backend/eval/eval_dataset_SW_service_context.json\")\n",
    "    # print(f\"Saved evaluation dataset at: /workspaces/sec-insights/backend/eval/eval_dataset_SW_service_context.json\")\n",
    "\n",
    "else: \n",
    "    print(f\"Evaluation dataset already exists at: {file_path}\")\n",
    "    eval_dataset = QueryResponseDataset.from_json(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Query Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_='''\n",
    "Note - The following code snippet had to be added to the CorrectnessEvaluator class within the llama-index v\"0.9.7\" package at\n",
    "llama_index/evaluation/correctness.py before line: score_str, reasoning_str = eval_response.split(\"\\n\", 1)\n",
    "This resolves an issue where eval_resonse is created beginning with a newline character, resulting in an error trying to convert\n",
    "an empty str to a float on line: score = float(score_str).\n",
    "\n",
    "Code snippet:\n",
    "if eval_response[0] == '\\n':\n",
    "    eval_response = eval_response[1:]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluate responses based on the following metrics:\n",
    "    Correctness:            The correctness of a response - A score between 1 (worst) and 5 (best).\n",
    "    Semantic Similarity:    The similarity between embeddings of the generated answer and reference answer.\n",
    "    Relevance:              The relevance of retrieved context and response to the query. Considers the query string, retrieved context, and response string.\n",
    "    Faithfulness:           How well the response is supported by the retrieved context (i.e., Is there hallucination?)\n",
    "'''\n",
    "from llama_index.evaluation import CorrectnessEvaluator, SemanticSimilarityEvaluator, RelevancyEvaluator, FaithfulnessEvaluator, BatchEvalRunner\n",
    "from llama_index.evaluation.eval_utils import get_responses\n",
    "\n",
    "evaluator_c = CorrectnessEvaluator()\n",
    "evaluator_s = SemanticSimilarityEvaluator()\n",
    "evaluator_r = RelevancyEvaluator()\n",
    "evaluator_f = FaithfulnessEvaluator()\n",
    "\n",
    "evaluator_dict = {\n",
    "    \"correctness\": evaluator_c,\n",
    "    \"faithfulness\": evaluator_f,\n",
    "    \"relevancy\": evaluator_r,\n",
    "    \"semantic_similarity\": evaluator_s,\n",
    "}\n",
    "batch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)\n",
    "\n",
    "eval_qs = eval_dataset.questions\n",
    "ref_response_strs = [r for (_, r) in eval_dataset.qr_pairs]\n",
    "print(f\"{len(eval_qs)} questions in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Responses from Each Query Engine for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 35\n",
    "\n",
    "import time\n",
    "time.sleep(12)\n",
    "print(f\"Original Query Engine\")\n",
    "original_pred_responses = get_responses(\n",
    "    eval_qs[:max_samples], original_query_engine, show_progress=True\n",
    ")\n",
    "time.sleep(12)\n",
    "print(f\"Sentence-Window Query Engine\")\n",
    "sentence_window_pred_responses = get_responses(\n",
    "    eval_qs[:max_samples], sentence_window_query_engine, show_progress=True\n",
    ")\n",
    "time.sleep(12)\n",
    "print(f\"Auto-Merging Query Engine\")\n",
    "auto_merging_pred_responses = get_responses(\n",
    "    eval_qs[:max_samples], auto_merging_query_engine, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Query Engine\")\n",
    "original_pred_responses = get_responses(\n",
    "    eval_qs[:max_samples], original_query_engine, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sentence-Window Query Engine\")\n",
    "sentence_window_pred_responses = get_responses(\n",
    "    eval_qs[:max_samples], sentence_window_query_engine, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Auto-Merging Query Engine\")\n",
    "auto_merging_pred_responses = get_responses(\n",
    "    eval_qs[:max_samples], auto_merging_query_engine, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Responses from Each Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Query Engine\")\n",
    "original_eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs[:max_samples],\n",
    "    responses=original_pred_responses[:max_samples],\n",
    "    reference=ref_response_strs[:max_samples],\n",
    ")\n",
    "print(f\"Sentence-Window Query Engine\")\n",
    "sentence_window_eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs[:max_samples],\n",
    "    responses=sentence_window_pred_responses[:max_samples],\n",
    "    reference=ref_response_strs[:max_samples],\n",
    ")\n",
    "print(f\"Auto-Merging Query Engine\")\n",
    "auto_merging_eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs[:max_samples],\n",
    "    responses=auto_merging_pred_responses[:max_samples],\n",
    "    reference=ref_response_strs[:max_samples],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation.eval_utils import get_results_df\n",
    "results_df = get_results_df(\n",
    "    [original_eval_results, sentence_window_eval_results, auto_merging_eval_results],\n",
    "    [\"Base Retriever\", \"Sentence-Window Retriever\", \"Auto-Merging Retriever\"],\n",
    "    [\"correctness\", \"relevancy\", \"faithfulness\", \"semantic_similarity\"],\n",
    ")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to CSV file\n",
    "OUTPUT_PATH = '/workspaces/sec-insights/backend/eval/results.csv'\n",
    "results_df.to_csv(OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.node_parser import SentenceWindowNodeParser\n",
    "# from llama_index.callbacks.base import BaseCallbackHandler, CallbackManager\n",
    "# callback_manager = CallbackManager([callback_handler])\n",
    "# sw_parser = SentenceWindowNodeParser.from_defaults(\n",
    "#     window_size=3,\n",
    "#     window_metadata_key=\"window\",\n",
    "#     original_text_metadata_key=\"original_text\",\n",
    "#     callback_manager=callback_manager,\n",
    "# )\n",
    "# llm = OpenAI(\n",
    "#     temperature=0,\n",
    "#     model=\"gpt-3.5-turbo-0613\",\n",
    "#     streaming=False,\n",
    "# )\n",
    "# from llama_index.embeddings.openai import OpenAIEmbedding, OpenAIEmbeddingMode, OpenAIEmbeddingModelType\n",
    "# embedding_model = OpenAIEmbedding(\n",
    "#     mode=OpenAIEmbeddingMode.SIMILARITY_MODE,\n",
    "#     model_type=OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002,\n",
    "# )\n",
    "# sw_service_context = ServiceContext.from_defaults(\n",
    "#     callback_manager=[callback_handler],\n",
    "#     llm=llm,\n",
    "#     embed_model=embedding_model,\n",
    "#     node_parser=sw_parser,\n",
    "# )\n",
    "# sw_index = VectorStoreIndex.from_documents(\n",
    "#     document,\n",
    "#     service_context=sw_service_context,\n",
    "# )\n",
    "# sw_postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "# sw_rerank = LLMRerank(\n",
    "#     top_n=4,\n",
    "#     service_context=sw_service_context,\n",
    "# )\n",
    "# sw_query_engine = sw_index.as_query_engine(\n",
    "#     similarity_top_k=8, node_postprocessors=[sw_postproc, sw_rerank]\n",
    "# )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
